{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc79690-254e-4312-8670-1e5bff3b28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from solipsism.core.context import Context\n",
    "from solipsism.core.system import System\n",
    "from solipsism.core.llm import GeminiLLM\n",
    "from solipsism.tools.file_io import (\n",
    "    ListFilesTool,\n",
    "    ReadFileTool,\n",
    "    WriteFileTool,\n",
    "    CreateDirectoryTool,\n",
    "    MoveItemTool,\n",
    "    DeleteItemTool\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    \"\"\"\n",
    "    Solipsismアプリケーションのメインエントリポイント。\n",
    "    コンポーネントを初期化し、メインの対話コンテクストを開始します。\n",
    "    \"\"\"\n",
    "    # 1. アプリケーション全体のロギングを設定\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s.%(funcName)s - %(levelname)s - %(message)s',\n",
    "        stream=sys.stdout,\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Starting Solipsism application...\")\n",
    "\n",
    "    # 2. 主要なコンポーネントをインスタンス化\n",
    "    llm = GeminiLLM()\n",
    "    system = System()\n",
    "\n",
    "    # 3. 利用可能なすべてのツールをインスタンス化し、Systemに登録\n",
    "    #    各ツールには、セキュリティサンドボックスとして機能するルートパスを指定します。\n",
    "    logger.info(\"Initializing and registering tools...\")\n",
    "    tools_to_register = [\n",
    "        ListFilesTool(root_path=\"../\"),\n",
    "        ReadFileTool(root_path=\"../\"),\n",
    "        WriteFileTool(root_path=\"../\"),\n",
    "        CreateDirectoryTool(root_path=\"../\"),\n",
    "        MoveItemTool(root_path=\"../\"),\n",
    "        DeleteItemTool(root_path=\"../\"),\n",
    "    ]\n",
    "    for tool in tools_to_register:\n",
    "        system.add_tool(tool)\n",
    "\n",
    "    # 4. メインのContextを作成\n",
    "    #    LLM、System、そしてエージェントの振る舞いを定義する\n",
    "    #    ベースプロンプトのパスを渡します。\n",
    "    try:\n",
    "        context = Context(\n",
    "            llm=llm,\n",
    "            system=system,\n",
    "            base_prompt_path=\"../solipsism/prompts/root_prompt.lpml\",\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to initialize Context: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # 5. Contextに対話を開始させるための最初のタスクを与える\n",
    "    #    ベースプロンプトのルールに基づき、LLMはこのタスクを起点に行動を開始します。\n",
    "    initial_task = (\n",
    "        \"You are an AI assistant in a new context. \"\n",
    "        \"Your goal is to understand the project and fulfill your role. \"\n",
    "        \"Please begin your workflow.\"\n",
    "    )\n",
    "    await context.start(\n",
    "        initial_task=initial_task,\n",
    "        max_turns=100\n",
    "    )\n",
    "\n",
    "    logger.info(\"Solipsism application has finished its run.\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(main())\n",
    "#     except KeyboardInterrupt:\n",
    "#         logging.info(\"Application interrupted by user. Exiting...\")\n",
    "#     except Exception as e:\n",
    "#         logging.critical(f\"An unhandled error occurred: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66c5471c-63a0-4697-9671-b852c6be2619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-02 09:20:24,963 - __main__.main - INFO - Starting Solipsism application...\n",
      "2025-09-02 09:20:24,999 - solipsism.core.system.__init__ - INFO - System initialized.\n",
      "2025-09-02 09:20:24,999 - __main__.main - INFO - Initializing and registering tools...\n",
      "2025-09-02 09:20:24,999 - solipsism.tools.file_io.__init__ - INFO - ListFilesTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:24,999 - solipsism.tools.file_io.__init__ - INFO - ReadFileTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:25,000 - solipsism.tools.file_io.__init__ - INFO - WriteFileTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:25,000 - solipsism.tools.file_io.__init__ - INFO - CreateDirectoryTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:25,000 - solipsism.tools.file_io.__init__ - INFO - MoveItemTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:25,000 - solipsism.tools.file_io.__init__ - INFO - DeleteItemTool initialized with root: /home/ryamauchi/projects/solipsism\n",
      "2025-09-02 09:20:25,000 - solipsism.core.system.add_tool - INFO - Tool 'list_files' has been added.\n",
      "2025-09-02 09:20:25,000 - solipsism.core.system.add_tool - INFO - Tool 'read_file' has been added.\n",
      "2025-09-02 09:20:25,000 - solipsism.core.system.add_tool - INFO - Tool 'write_file' has been added.\n",
      "2025-09-02 09:20:25,000 - solipsism.core.system.add_tool - INFO - Tool 'create_directory' has been added.\n",
      "2025-09-02 09:20:25,001 - solipsism.core.system.add_tool - INFO - Tool 'move_item' has been added.\n",
      "2025-09-02 09:20:25,001 - solipsism.core.system.add_tool - INFO - Tool 'delete_item' has been added.\n",
      "2025-09-02 09:20:25,001 - solipsism.core.context.__init__ - INFO - Base prompt loaded from ../solipsism/prompts/root_prompt.lpml\n",
      "2025-09-02 09:20:25,001 - solipsism.core.context.start - INFO - Starting context with initial task: 'You are an AI assistant in a new context. Your goal is to understand the project and fulfill your role. Please begin your workflow.'\n",
      "2025-09-02 09:20:25,001 - solipsism.core.context.start - INFO - --- Turn 1/100 ---\n",
      "2025-09-02 09:20:25,001 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:26,685 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:26,687 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "<thinking label=\"Initial Workflow Step\">\n",
      "The first step in the workflow rule states that I must read the file `./solipsism/notes/readme.md`. I will use the `read_file` tag for this purpose.\n",
      "</thinking>\n",
      "<read_file path=\"./solipsism/notes/readme.md\"/>\n",
      "2025-09-02 09:20:26,688 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:26,689 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <read_file>. Scheduling execution.\n",
      "2025-09-02 09:20:26,690 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:26,690 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:27,692 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:27,693 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"read_file\" path=\"./solipsism/notes/readme.md\">\n",
      "本プロジェクトは実装中です。\n",
      "全体像を把握してみなさい。\n",
      "\n",
      "まずは以下を読むのが良いだろう。\n",
      "- ./solipsism/core/context.py\n",
      "- ./solipsism/core/system.py\n",
      "- ./solipsism/core/llm.py\n",
      "\n",
      "その上でコンテクストディレクトリに概要をまとめたマークダウンを作れ。\n",
      "\n",
      "</output>\n",
      "2025-09-02 09:20:27,694 - solipsism.core.context.start - INFO - --- Turn 2/100 ---\n",
      "2025-09-02 09:20:27,695 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:29,347 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:29,349 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Understanding the Task and Next Steps\">\n",
      "I have read the `readme.md` file. The file indicates that the project is under implementation and instructs me to grasp the overall picture. It suggests reading three specific files:\n",
      "1. `./solipsism/core/context.py`\n",
      "2. `./solipsism/core/system.py`\n",
      "3. `./solipsism/core/llm.py`\n",
      "\n",
      "After reading these files, I need to create a markdown summary in the context directory. The context directory for this specific context (ID `99b3d798`) should be `./workspace/context/99b3d798`. I will create a markdown file there, for example, `summary.md`.\n",
      "\n",
      "My next step is to read the first suggested file: `./solipsism/core/context.py`.\n",
      "</thinking>\n",
      "<read_file path=\"./solipsism/core/context.py\"/>\n",
      "\n",
      "2025-09-02 09:20:29,349 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:29,350 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <read_file>. Scheduling execution.\n",
      "2025-09-02 09:20:29,351 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:29,351 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:30,353 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:30,354 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"read_file\" path=\"./solipsism/core/context.py\">\n",
      "import asyncio\n",
      "import logging\n",
      "import hashlib\n",
      "import uuid\n",
      "from datetime import datetime, timezone\n",
      "from enum import Enum, auto\n",
      "from typing import List\n",
      "import re\n",
      "from .llm import BaseLLM\n",
      "from .system import System\n",
      "from .lpml import Element, LPMLTree, deparse, parse, findall\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "ConversationHistory = List[Element]\n",
      "\n",
      "\n",
      "class ContextState(Enum):\n",
      "    \"\"\"コンテクストの状態を管理する列挙型\"\"\"\n",
      "    IDLE = auto()\n",
      "    RUNNING = auto()\n",
      "    WAITING = auto()\n",
      "    TERMINATED = auto()\n",
      "\n",
      "\n",
      "def _generate_id():\n",
      "    unique_id = uuid.uuid4()\n",
      "    hash_object = hashlib.sha256(str(unique_id).encode())\n",
      "    return hash_object.hexdigest()[:8]\n",
      "\n",
      "\n",
      "class Context:\n",
      "    \"\"\"\n",
      "    会話履歴と状態を管理し、LLMとSystem間の対話ループを統括するクラス。\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(self, llm: BaseLLM, system: System, base_prompt_path: str):\n",
      "        self.id = _generate_id()\n",
      "        self.llm = llm\n",
      "        self.system = system\n",
      "        self.conversation_history: ConversationHistory = []\n",
      "        self.state = ContextState.IDLE\n",
      "        self.turn_count = 0\n",
      "\n",
      "        try:\n",
      "            with open(base_prompt_path, 'r', encoding='utf-8') as f:\n",
      "                self.base_prompt = f.read()\n",
      "            logger.info(f\"Base prompt loaded from {base_prompt_path}\")\n",
      "        except FileNotFoundError:\n",
      "            logger.error(f\"Base prompt file not found at: {base_prompt_path}\")\n",
      "            self.base_prompt = \"<error>Base prompt not found.</error>\"\n",
      "\n",
      "    @property\n",
      "    def prompt(self):\n",
      "        return self.base_prompt + \"\\n\\n\" + self.system.get_tool_definitions()\n",
      "\n",
      "    def _get_timestamp(self) -> str:\n",
      "        \"\"\"ISO 8601形式のUTCタイムスタンプを返す。\"\"\"\n",
      "        return datetime.now(timezone.utc).isoformat()\n",
      "\n",
      "    def _add_to_history(self, tag: str, content: str):\n",
      "        \"\"\"タイムスタンプとターン数を付加して履歴に要素を追加する。\"\"\"\n",
      "        element = {\n",
      "            \"tag\": tag,\n",
      "            \"attributes\": {\n",
      "                \"turn\": str(self.turn_count),\n",
      "                \"timestamp\": self._get_timestamp()\n",
      "            },\n",
      "            \"content\": \"\\n\" + content + \"\\n\"\n",
      "        }\n",
      "        self.conversation_history.append(element)\n",
      "\n",
      "    def _sanitize_llm_response(self, lpml_string: str) -> str:\n",
      "        \"\"\"\n",
      "        LLMの応答をサニタイズするメインメソッド。\n",
      "        \"\"\"\n",
      "        # assistant tag を除去\n",
      "        cleaned = re.sub(r'<assistant[^>]+>', '', lpml_string)\n",
      "        cleaned = re.sub(r'</assistant>', '', cleaned)\n",
      "        return cleaned\n",
      "\n",
      "    def _build_full_prompt(self) -> str:\n",
      "        \"\"\"\n",
      "        ベースプロンプトと会話履歴から、LLMに渡す最終的なプロンプト文字列を構築する。\n",
      "        \"\"\"\n",
      "        # ベースプロンプトは常にプロンプトの土台となる\n",
      "        # 会話履歴はLPML要素のリストなので、deparseで文字列に変換\n",
      "        _history = sum(\n",
      "            [[x, '\\n\\n'] for x in self.conversation_history], [])[:-1]\n",
      "        history_str = f\"<log>\\n{deparse(_history)}\\n</log>\"\n",
      "        return f\"{self.prompt}\\n\\n{history_str}\"\n",
      "\n",
      "    async def start(self, initial_task: str = None, max_turns: int = 10):\n",
      "        \"\"\"対話ループを開始する\"\"\"\n",
      "        if self.state != ContextState.IDLE:\n",
      "            logger.warning(\"Context is already running or finished.\")\n",
      "            return\n",
      "\n",
      "        logger.info(f\"Starting context with initial task: '{initial_task}'\")\n",
      "        self.state = ContextState.RUNNING\n",
      "\n",
      "        initial_message = f\"context id: {self.id}\"\n",
      "        if initial_task is not None:\n",
      "            initial_message += f\"\\nTask: {initial_task}\"\n",
      "\n",
      "        # 最初のタスクを<system>メッセージとして履歴に追加\n",
      "        self.turn_count = 1\n",
      "        self._add_to_history(\"system\", initial_message)\n",
      "\n",
      "        while self.turn_count <= max_turns and self.state == ContextState.RUNNING:\n",
      "            logger.info(f\"--- Turn {self.turn_count}/{max_turns} ---\")\n",
      "\n",
      "            # 1. プロンプトを構築し、LLMに応答を生成させる\n",
      "            prompt_str = self._build_full_prompt()\n",
      "            llm_response_str = await self.llm.generate(prompt_str)\n",
      "            \n",
      "            # 2. LLMの応答をサニタイズ\n",
      "            sanitized_response = self._sanitize_llm_response(llm_response_str)\n",
      "            logger.info(f\"Assistant Response:\\n{sanitized_response}\")\n",
      "            self._add_to_history(\"assistant\", sanitized_response)\n",
      "\n",
      "            # 3. 応答に<wait>タグが含まれているかチェック\n",
      "            if \"<wait>\" in sanitized_response and False:\n",
      "                logger.info(\"'<wait>' tag found. Context is entering WAITING state.\")\n",
      "                self.state = ContextState.WAITING\n",
      "                break\n",
      "\n",
      "            # 4. Systemにツール実行を依頼\n",
      "            num_tasks = await self.system.process_llm_output(sanitized_response)\n",
      "\n",
      "            # 5. ツールが実行された場合、結果を待って次のターンへ\n",
      "            if num_tasks > 0:\n",
      "                logger.info(\"Waiting for tool results...\")\n",
      "                while True:\n",
      "                    await asyncio.sleep(1)\n",
      "                    tool_results_lpml = await self.system.get_tool_results_as_lpml()\n",
      "                    if tool_results_lpml:\n",
      "                        logger.info(f\"System Response (Tool Results):\\n{tool_results_lpml}\")\n",
      "                        self.turn_count += 1\n",
      "                        self._add_to_history(\"system\", tool_results_lpml)\n",
      "                        break # ポーリングループを抜け、次のLLMターンへ\n",
      "            else:\n",
      "                # ツール実行がなければ、LLMが思考を完了したとみなし待機状態へ\n",
      "                logger.info(\"No tools were executed. Entering WAITING state.\")\n",
      "                self.state = ContextState.WAITING\n",
      "                break\n",
      "        \n",
      "        if self.state == ContextState.RUNNING:\n",
      "            logger.warning(f\"Max turns ({max_turns}) reached. Setting state to TERMINATED.\")\n",
      "            self.state = ContextState.TERMINATED\n",
      "\n",
      "        logger.info(f\"--- Context loop finished with state: {self.state.name} ---\")\n",
      "</output>\n",
      "2025-09-02 09:20:30,355 - solipsism.core.context.start - INFO - --- Turn 3/100 ---\n",
      "2025-09-02 09:20:30,356 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:33,750 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:33,751 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Analyzing Context.py\">\n",
      "I have read `context.py`. This file defines the `Context` class, which is central to the project.\n",
      "\n",
      "Here's a breakdown of its key responsibilities:\n",
      "- **Manages Conversation History**: Stores `system` and `assistant` messages using `ConversationHistory` (a list of `Element` objects, which are LPML elements).\n",
      "- **Handles Context State**: Uses `ContextState` (IDLE, RUNNING, WAITING, TERMINATED) to manage its lifecycle.\n",
      "- **LLM Interaction**: Builds the full prompt for the LLM by combining a `base_prompt` and the `conversation_history`, and then sanitizes the LLM's response.\n",
      "- **System Interaction**: Passes the LLM's sanitized output to the `System` for tool execution. It then polls the `System` for tool results and adds them to the history.\n",
      "- **Turn Management**: Increments `turn_count` and manages the `max_turns` limit.\n",
      "- **LPML Integration**: Uses `LPMLTree`, `deparse`, and `parse` for handling LPML messages.\n",
      "\n",
      "Specifically, the `start` method outlines the core dialogue loop:\n",
      "1. Initialize with `context id` and `Task`.\n",
      "2. Build prompt and get LLM response.\n",
      "3. Sanitize LLM response (removes `<assistant>` tags).\n",
      "4. Check for `<wait>` tag (though currently commented out in the provided code snippet `and False`).\n",
      "5. Process LLM output for tool execution via `system.process_llm_output`.\n",
      "6. If tools were executed, poll for results (`system.get_tool_results_as_lpml`) and add them to history.\n",
      "7. If no tools were executed, the context enters a `WAITING` state.\n",
      "8. The loop continues until `max_turns` is reached or the context enters a `WAITING` or `TERMINATED` state.\n",
      "\n",
      "My next step is to read the second suggested file: `./solipsism/core/system.py`.\n",
      "</thinking>\n",
      "<read_file path=\"./solipsism/core/system.py\"/>\n",
      "\n",
      "2025-09-02 09:20:33,752 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:33,752 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <read_file>. Scheduling execution.\n",
      "2025-09-02 09:20:33,752 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:33,753 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:34,754 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:34,755 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"read_file\" path=\"./solipsism/core/system.py\">\n",
      "import asyncio\n",
      "import logging\n",
      "from typing import Dict, List, Optional\n",
      "from .tool import BaseTool\n",
      "from .lpml import parse, findall, deparse, Element\n",
      "\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class System:\n",
      "    \"\"\"LLMの出力に応じたツールの実行を管理するクラス\"\"\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.tools: Dict[str, BaseTool] = {}\n",
      "        self.result_queue: asyncio.Queue[Element] = asyncio.Queue()\n",
      "        logger.info(\"System initialized.\")\n",
      "\n",
      "    def add_tool(self, tool: BaseTool):\n",
      "        \"\"\"システムにツールを登録する\"\"\"\n",
      "        if tool.name in self.tools:\n",
      "            logger.warning(\n",
      "                f\"Tool '{tool.name}' is already registered. Overwriting.\")\n",
      "        self.tools[tool.name] = tool\n",
      "        logger.info(f\"Tool '{tool.name}' has been added.\")\n",
      "\n",
      "    def get_tool_definitions(self):\n",
      "        definitions = \"\"\n",
      "        for name, tool in self.tools.items():\n",
      "            definitions += tool.definition + \"\\n\\n\"\n",
      "        return definitions.strip()\n",
      "\n",
      "    async def process_llm_output(self, lpml_string: str) -> int:\n",
      "        \"\"\"\n",
      "        LLMの出力をパースし、対応するツールを実行する。\n",
      "        実行をスケジュールしたタスクの数を返す。\n",
      "        \"\"\"\n",
      "        logger.info(\"Processing LLM output for tool execution...\")\n",
      "        try:\n",
      "            exclude = [\"define_tag\", \"rule\", \"send\", \"code\"]\n",
      "            exclude += list(self.tools.keys())\n",
      "            tree = parse(lpml_string, exclude=exclude)\n",
      "        except Exception as e:\n",
      "            logger.error(f\"Failed to parse LPML string: {e}\", exc_info=True)\n",
      "            return 0\n",
      "\n",
      "        tasks_to_run = []\n",
      "        for tag_name, tool in self.tools.items():\n",
      "            tool_elements = findall(tree, tag_name)\n",
      "            for element in tool_elements:\n",
      "                logger.info(f\"Found tool tag: <{tag_name}>. Scheduling execution.\")\n",
      "                task = asyncio.create_task(tool.run(element, self.result_queue))\n",
      "                tasks_to_run.append(task)\n",
      "\n",
      "        if tasks_to_run:\n",
      "            num_tasks = len(tasks_to_run)\n",
      "            logger.info(f\"Scheduled {num_tasks} tool(s) to run in the background.\")\n",
      "            return num_tasks\n",
      "        else:\n",
      "            logger.info(\"No tool tags found in the LLM output.\")\n",
      "            return 0\n",
      "\n",
      "    async def get_tool_results_as_lpml(self) -> Optional[str]:\n",
      "        \"\"\"\n",
      "        結果キューに溜まったツール実行結果をLPML文字列として取得する。\n",
      "        キューが空の場合はNoneを返す。\n",
      "        \"\"\"\n",
      "        if self.result_queue.empty():\n",
      "            return None\n",
      "\n",
      "        results: List[Element] = []\n",
      "        while not self.result_queue.empty():\n",
      "            result = await self.result_queue.get()\n",
      "            results.append(result)\n",
      "            self.result_queue.task_done()\n",
      "\n",
      "        if results:\n",
      "            logger.info(f\"Drained {len(results)} tool result(s) from the queue.\")\n",
      "            _results = sum([[x, '\\n\\n'] for x in results], [])[:-1]\n",
      "            return deparse(_results)\n",
      "\n",
      "        return None\n",
      "\n",
      "</output>\n",
      "2025-09-02 09:20:34,756 - solipsism.core.context.start - INFO - --- Turn 4/100 ---\n",
      "2025-09-02 09:20:34,757 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:37,845 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:37,847 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Analyzing System.py\">\n",
      "I have read `system.py`. This file defines the `System` class, which is responsible for managing and executing tools based on the LLM's output.\n",
      "\n",
      "Here's a breakdown of its key responsibilities:\n",
      "- **Tool Management**: Registers `BaseTool` instances via `add_tool`. It stores these tools in a dictionary, mapping tag names to tool instances.\n",
      "- **Tool Definitions**: Provides a method `get_tool_definitions` to retrieve the LPML definitions of all registered tools, which are likely incorporated into the LLM's prompt.\n",
      "- **Tool Execution**: The `process_llm_output` method is crucial. It parses the LLM's LPML output, identifies tool tags (excluding certain tags like `define_tag`, `rule`, `send`, `code`, and the tool names themselves from the initial parsing to avoid self-referencing issues), and then schedules the execution of these tools as `asyncio` tasks.\n",
      "- **Result Queue**: Uses an `asyncio.Queue` (`result_queue`) to collect the results of tool executions. Each tool's `run` method is expected to put its result (an `Element`) into this queue.\n",
      "- **Result Retrieval**: The `get_tool_results_as_lpml` method retrieves all available results from the `result_queue`, converts them into an LPML string, and returns them. If the queue is empty, it returns `None`.\n",
      "\n",
      "In essence, the `System` acts as the intermediary between the `Context` (which gets output from the LLM) and the actual tools. It takes the LLM's instructions (in LPML), executes the corresponding functions, and then provides the results back to the `Context` for the next turn.\n",
      "\n",
      "My next step is to read the third suggested file: `./solipsism/core/llm.py`.\n",
      "</thinking>\n",
      "<read_file path=\"./solipsism/core/llm.py\"/>\n",
      "\n",
      "2025-09-02 09:20:37,848 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:37,848 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <read_file>. Scheduling execution.\n",
      "2025-09-02 09:20:37,849 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:37,849 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:38,851 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:38,852 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"read_file\" path=\"./solipsism/core/llm.py\">\n",
      "import os\n",
      "import re\n",
      "import asyncio\n",
      "import logging\n",
      "from abc import ABC, abstractmethod\n",
      "from google import genai\n",
      "from dotenv import load_dotenv\n",
      "\n",
      "\n",
      "load_dotenv()\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "class BaseLLM(ABC):\n",
      "    \"\"\"\n",
      "    すべてのLLM実装のための抽象基底クラス。\n",
      "    LLMからの応答を生成するための共通インターフェースを定義します。\n",
      "    \"\"\"\n",
      "\n",
      "    @abstractmethod\n",
      "    async def generate(self, prompt_str: str) -> str:\n",
      "        \"\"\"\n",
      "        与えられたプロンプトに基づいてLLMからの応答を生成します。\n",
      "\n",
      "        このメソッドは、具象クラス（例: MockLLM, GeminiLLM）で必ず実装されなければなりません。\n",
      "\n",
      "        :param prompt_str: LLMに送信する完全なプロンプト文字列。\n",
      "        :return: LLMの応答文字列。\n",
      "        \"\"\"\n",
      "        pass\n",
      "\n",
      "\n",
      "class GeminiLLM(BaseLLM):\n",
      "\n",
      "    def __init__(self, model=None, temperature=0.7, thinking_budget=0):\n",
      "        self.client = genai.Client(api_key=os.environ.get(\"GEMINI_API\"))\n",
      "        self.model = model or \"gemini-2.5-flash\"\n",
      "        self.temperature = temperature\n",
      "        self.thinking_budget = thinking_budget\n",
      "        self.files = []\n",
      "\n",
      "    def upload_file(self, path, name=None, mime_type=None):\n",
      "        file = self.client.files.upload(\n",
      "            file=path, config=genai.types.UploadFileConfig(\n",
      "                mimeType=mime_type,\n",
      "                displayName=path)\n",
      "        )\n",
      "        self.files.append(file)\n",
      "        return file\n",
      "\n",
      "    async def generate(self, prompt: str) -> str:        \n",
      "        response = await self.client.aio.models.generate_content(\n",
      "            model=self.model,\n",
      "            contents=[prompt] + self.files,\n",
      "            config=genai.types.GenerateContentConfig(\n",
      "                thinking_config=genai.types.ThinkingConfig(\n",
      "                    thinking_budget=self.thinking_budget),\n",
      "                temperature=self.temperature\n",
      "            ),\n",
      "        )\n",
      "        return response.text\n",
      "\n",
      "</output>\n",
      "2025-09-02 09:20:38,853 - solipsism.core.context.start - INFO - --- Turn 5/100 ---\n",
      "2025-09-02 09:20:38,854 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:42,966 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:42,967 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Analyzing LLM.py\">\n",
      "I have read `llm.py`. This file defines the `BaseLLM` abstract class and a concrete implementation `GeminiLLM`. This module is responsible for interacting with Large Language Models.\n",
      "\n",
      "Here's a breakdown of its key responsibilities:\n",
      "- **Abstract Base Class `BaseLLM`**: Defines a common interface (`generate` method) for all LLM implementations, ensuring consistency.\n",
      "- **`GeminiLLM` Implementation**:\n",
      "    - Initializes with a Gemini API key (loaded from environment variables), model name, temperature, and thinking budget.\n",
      "    - Provides an `upload_file` method, which suggests that the LLM can handle multi-modal inputs (e.g., files in addition to text prompts). This is an interesting feature.\n",
      "    - The core `generate` method asynchronously calls the Gemini API to generate content based on the provided `prompt` and any `files` that have been uploaded.\n",
      "\n",
      "In summary, `llm.py` handles the actual communication with the underlying LLM, taking the constructed prompt and returning the LLM's raw text response.\n",
      "\n",
      "**Overall Project Understanding:**\n",
      "Now that I have read `readme.md`, `context.py`, `system.py`, and `llm.py`, I have a good grasp of the project's overall architecture and how the components interact:\n",
      "\n",
      "1.  **`Context`**: Orchestrates the entire dialogue loop. It constructs prompts, sends them to the `LLM`, receives responses, sanitizes them, and passes them to the `System` for tool execution. It manages the conversation history and context state.\n",
      "2.  **`System`**: Acts as the tool manager. It registers available tools, parses the LLM's output to identify and execute tool calls asynchronously, and collects the results in a queue. It then provides these results back to the `Context`.\n",
      "3.  **`LLM`**: Provides the interface for interacting with the Large Language Model (e.g., Gemini). It takes the prompt from the `Context` and returns the LLM's generated response. It can also handle multi-modal inputs.\n",
      "\n",
      "The workflow is a continuous loop: `Context` -> `LLM` -> `Context` -> `System` (tool execution) -> `System` (results) -> `Context` (next turn).\n",
      "\n",
      "**Next Step:**\n",
      "The `readme.md` instructed me to \"その上でコンテクストディレクトリに概要をまとめたマークダウンを作れ。\" (Then, create a markdown summary in the context directory).\n",
      "\n",
      "My context ID is `99b3d798`. Therefore, the context directory is `./workspace/context/99b3d798`. I should create a markdown file, for example, `summary.md`, within this directory.\n",
      "\n",
      "Before writing the file, I should ensure the directory exists. I will use `create_directory` for that. Then I will use `write_file` to create the markdown summary.\n",
      "</thinking>\n",
      "<create_directory path=\"./workspace/context/99b3d798\" parents=\"true\"/>\n",
      "\n",
      "2025-09-02 09:20:42,968 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:42,968 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <create_directory>. Scheduling execution.\n",
      "2025-09-02 09:20:42,969 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:42,969 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:43,971 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:43,972 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"create_directory\" path=\"./workspace/context/99b3d798\" parents=\"true\">\n",
      "Successfully created directory './workspace/context/99b3d798'.\n",
      "</output>\n",
      "2025-09-02 09:20:43,972 - solipsism.core.context.start - INFO - --- Turn 6/100 ---\n",
      "2025-09-02 09:20:43,975 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:50,132 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:50,133 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Creating Summary Markdown File\">\n",
      "I have successfully created the directory `./workspace/context/99b3d798`. Now I need to create the `summary.md` file within this directory, summarizing my understanding of the project. I will use the `write_file` tag for this.\n",
      "</thinking>\n",
      "<write_file path=\"./workspace/context/99b3d798/summary.md\" mode=\"overwrite\">\n",
      "# Project Solipsism Overview\n",
      "\n",
      "This document summarizes the core components and their interactions within the `solipsism` project, based on the `readme.md`, `context.py`, `system.py`, and `llm.py` files.\n",
      "\n",
      "## 1. Project Goal\n",
      "The `solipsism` project aims to be a context orchestration system, where pairs of AI assistants and systems form \"contexts\" that collaborate to solve tasks. It generates and manages various contexts.\n",
      "\n",
      "## 2. Core Components\n",
      "\n",
      "### 2.1. `Context` (Defined in `solipsism/core/context.py`)\n",
      "The `Context` class is the central orchestrator of the dialogue loop. Its primary responsibilities include:\n",
      "-   **Conversation Management**: Maintains a history of interactions between the `system` and `assistant` as LPML elements.\n",
      "-   **State Management**: Manages its lifecycle through states like `IDLE`, `RUNNING`, `WAITING`, and `TERMINATED`.\n",
      "-   **LLM Interaction**: Constructs the complete prompt for the LLM (combining a base prompt and conversation history) and sanitizes the LLM's raw output.\n",
      "-   **System Integration**: Passes the sanitized LLM output to the `System` for tool execution and then polls the `System` for the results of these executions, incorporating them into the conversation history.\n",
      "-   **Turn Control**: Manages the progression of dialogue turns and enforces a maximum turn limit.\n",
      "-   **LPML Handling**: Utilizes LPML parsing and de-parsing utilities to manage structured communication.\n",
      "\n",
      "The `start` method encapsulates the main dialogue loop, which involves generating LLM responses, processing them with the System, and updating the conversation history.\n",
      "\n",
      "### 2.2. `System` (Defined in `solipsism/core/system.py`)\n",
      "The `System` class acts as the tool execution manager. Its key functions are:\n",
      "-   **Tool Registration**: Allows the registration of `BaseTool` instances, making them available for use.\n",
      "-   **Tool Definition Provision**: Provides LPML definitions of registered tools, which are integrated into the LLM's prompt to inform it about available functionalities.\n",
      "-   **LLM Output Processing**: Parses the LLM's LPML output to identify and schedule the asynchronous execution of tool calls.\n",
      "-   **Result Queue**: Uses an `asyncio.Queue` to collect results from executed tools.\n",
      "-   **Result Retrieval**: Retrieves accumulated tool execution results from the queue and formats them into an LPML string for the `Context`.\n",
      "\n",
      "The `System` effectively bridges the gap between the LLM's declarative instructions (in LPML) and the actual execution of functions.\n",
      "\n",
      "### 2.3. `LLM` (Defined in `solipsism/core/llm.py`)\n",
      "The `LLM` module provides the interface for interacting with Large Language Models.\n",
      "-   **`BaseLLM`**: An abstract base class defining a common `generate` method for all LLM implementations.\n",
      "-   **`GeminiLLM`**: A concrete implementation for Google's Gemini API. It handles:\n",
      "    -   Initialization with API keys, model, temperature, and thinking budget.\n",
      "    -   `upload_file` functionality, suggesting multi-modal input capabilities.\n",
      "    -   Asynchronously calls the Gemini API to generate text responses based on the provided prompt and any uploaded files.\n",
      "\n",
      "## 3. Workflow Summary\n",
      "The project operates in a continuous loop:\n",
      "1.  The `Context` prepares a prompt (base prompt + conversation history + tool definitions).\n",
      "2.  The `Context` sends the prompt to the `LLM` to generate a response.\n",
      "3.  The `LLM` returns a raw text response, which the `Context` sanitizes.\n",
      "4.  The `Context` passes the sanitized response to the `System`.\n",
      "5.  The `System` parses the response for tool calls, executes them asynchronously, and queues their results.\n",
      "6.  The `Context` polls the `System` for tool results.\n",
      "7.  Once results are available, the `Context` adds them to the conversation history, and the loop continues to the next turn, or enters a `WAITING` state if no tools were executed or a `<wait>` tag is encountered.\n",
      "\n",
      "This architecture enables AI assistants to interact with a system, execute tools, and solve tasks through a structured, LPML-based dialogue.\n",
      "</write_file>\n",
      "\n",
      "2025-09-02 09:20:50,134 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:50,134 - solipsism.core.system.process_llm_output - INFO - Found tool tag: <write_file>. Scheduling execution.\n",
      "2025-09-02 09:20:50,134 - solipsism.core.system.process_llm_output - INFO - Scheduled 1 tool(s) to run in the background.\n",
      "2025-09-02 09:20:50,134 - solipsism.core.context.start - INFO - Waiting for tool results...\n",
      "2025-09-02 09:20:51,136 - solipsism.core.system.get_tool_results_as_lpml - INFO - Drained 1 tool result(s) from the queue.\n",
      "2025-09-02 09:20:51,136 - solipsism.core.context.start - INFO - System Response (Tool Results):\n",
      "<output tool=\"write_file\" path=\"./workspace/context/99b3d798/summary.md\" mode=\"overwrite\">\n",
      "Successfully overwrote file './workspace/context/99b3d798/summary.md'.\n",
      "</output>\n",
      "2025-09-02 09:20:51,136 - solipsism.core.context.start - INFO - --- Turn 7/100 ---\n",
      "2025-09-02 09:20:51,137 - google_genai.models.generate_content - INFO - AFC is enabled with max remote calls: 10.\n",
      "2025-09-02 09:20:54,433 - httpx._send_single_request - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent \"HTTP/1.1 200 OK\"\n",
      "2025-09-02 09:20:54,434 - solipsism.core.context.start - INFO - Assistant Response:\n",
      "\n",
      "<thinking label=\"Completing the Task and Entering Wait State\">\n",
      "I have successfully created the summary markdown file in the specified context directory.\n",
      "According to the workflow rule, once the task is solved, I should enter a wait state. The current task was to understand the project and create a summary. I have completed this.\n",
      "Therefore, I will now use the `wait` tag to pause execution.\n",
      "</thinking>\n",
      "<wait/>\n",
      "\n",
      "2025-09-02 09:20:54,434 - solipsism.core.system.process_llm_output - INFO - Processing LLM output for tool execution...\n",
      "2025-09-02 09:20:54,435 - solipsism.core.system.process_llm_output - INFO - No tool tags found in the LLM output.\n",
      "2025-09-02 09:20:54,435 - solipsism.core.context.start - INFO - No tools were executed. Entering WAITING state.\n",
      "2025-09-02 09:20:54,435 - solipsism.core.context.start - INFO - --- Context loop finished with state: WAITING ---\n",
      "2025-09-02 09:20:54,436 - __main__.main - INFO - Solipsism application has finished its run.\n"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd735c-ca5c-400e-9824-722a5e0a4d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solipsism",
   "language": "python",
   "name": "solipsism"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
